{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-RNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "y5b6eSmPo2wv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6_86qsu1miV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoxlwImyzCJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5-sE96gy3YJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_orig=pd.read_csv('train.csv') \n",
        "X_test_orig= pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ss0jFLl3zJ-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = (X_train_orig.ix[:,1:].values).astype('float32')/255\n",
        "X_test = X_test_orig.values.astype('float32')/255\n",
        "Y_train_orig=X_train_orig.ix[:,0].values.astype('int32')\n",
        "Y_train= convert_to_one_hot(Y_train_orig, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ApELijr-ysYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 375, 375, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 375, 375, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ed6cgCw7zdO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
        "    X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0])\n",
        "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
        "    return X, Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31UgJwmO83V5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "hidden state dimension as well as input dimension are same i.e., 512 units "
      ]
    },
    {
      "metadata": {
        "id": "0Py3ggCntZRz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gru_cell(vt,h_prev):\n",
        "    Wvr = weight_variable([512,512])\n",
        "    Whr = weight_variable([512,512])\n",
        "    Br = bias_variable([1,512])\n",
        "    rt=tf.nn.sigmoid(tf.matmul(vt,Wvr)+tf.matmul(h_prev,Whr)+Br)\n",
        "    \n",
        "    Wvz = weight_variable([512,512])\n",
        "    Whz = weight_variable([512,512])\n",
        "    Bz = bias_variable([1,512])\n",
        "    zt=tf.nn.sigmoid(tf.matmul(vt,Wvz)+tf.matmul(h_prev,Whz)+Bz)\n",
        "    \n",
        "    Wvh_ = weight_variable([512,512])\n",
        "    Whh_ = weight_variable([512,512])\n",
        "    Bh_ = bias_variable([1,512])\n",
        "    t=tf.multiply(rt,h_prev)+Bh_\n",
        "    ht_=tf.nn.tanh(tf.matmul(vt,Wvh_)+tf.matmul(t,Whh_))\n",
        "    ht=tf.multiply(zt,h_prev)+tf.multiply(1-zt,ht_)\n",
        "    return ht"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dt_bplwvwSPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gru_rnn(fc1,fc2,fc3,fc4,fc5):\n",
        "      words=[fc1,fc2,fc3,fc4,fc5] #array of tensor\n",
        "      ht= tf.Variable(tf.zeros([None, 512]))\n",
        "      for x in range(5):\n",
        "           ht=gru_cell(words[i],ht)\n",
        "      return ht"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "22Y7DoHQhPXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation_cnn(X):\n",
        "    \"\"\"\n",
        "       Arguments :\n",
        "       X --input images of dimension (None, 375, 375, 3)\n",
        "       \n",
        "       Returns :\n",
        "       V : dictionary of visual features from the different branches with shape (None, 512)\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"branch1\"):\n",
        "        # Convolutional Layer #11 with relu acitvation\n",
        "        # ouput activation map of size 92*92*128\n",
        "        conv11 = tf.layers.conv2d(\n",
        "            inputs=input_layer,\n",
        "            filters=128,\n",
        "            kernel_size=[11, 11],\n",
        "            padding=\"same\",\n",
        "            stride=[4,4],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Convolutional Layer #12 with relu activation\n",
        "        # output activation map of size 92*92*128 \n",
        "        conv12 = tf.layers.conv2d(\n",
        "            inputs=conv11,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            strides=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Pooling Layer #1\n",
        "        # output activation map of size 45*45*128\n",
        "        pool1 = tf.layers.max_pooling2d(inputs=conv12, pool_size=[3, 3], strides=2, padding=\"same\")\n",
        "\n",
        "        # Add branch #1 here\n",
        "        # Convolutional Layer branch1conv with relu activation\n",
        "        # output activation map of size 45*45*128\n",
        "        branch1conv = tf.layers.conv2d(\n",
        "            inputs=pool1,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer 1 of brach 1\n",
        "        pol1_flat = tf.contrib.layers.flatten(branch1conv)\n",
        "        dense11 = tf.layers.dense(inputs=pool1_flat, units=1024, activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer 2 of brach 1\n",
        "        dense12 = tf.layers.dense(inputs=dense11, units=512, activation=tf.nn.relu)\n",
        "    \n",
        "    with tf.name_scope(\"branch2\"):\n",
        "        # Convolutional Layer #21\n",
        "        # output activation map of size 41*41*256\n",
        "        conv21 = tf.layers.conv2d(\n",
        "            inputs=pool1,\n",
        "            filters=256,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Convolutional Layer #21\n",
        "        # output activation map of size 41*41*256\n",
        "        conv22 = tf.layers.conv2d(\n",
        "            inputs=conv21,\n",
        "            filters=256,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            strides=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Pooling Layer #2\n",
        "        #output activation map of size 20*20*256\n",
        "        pool2 = tf.layers.max_pooling2d(inputs=conv22, pool_size=[3, 3], strides=2, padding=\"same\")\n",
        "\n",
        "        # Add branch #2 here\n",
        "        # Convolutional Layer branch2conv\n",
        "        # ouptut activation map of size 20*20*256\n",
        "        branch2conv = tf.layers.conv2d(\n",
        "            inputs=pool2,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #21\n",
        "        pool2_flat = tf.contrib.layers.flatten(branch2conv) \n",
        "        dense21 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #22\n",
        "        dense22 = tf.layers.dense(inputs=dense21, units=512, activation=tf.nn.relu)\n",
        "\n",
        "    with tf.name_scope(\"branch3\"):    \n",
        "        # Convolutional Layer #31\n",
        "        # ouput activation map of size 16*16*384\n",
        "        conv31 = tf.layers.conv2d(\n",
        "            inputs=pool2,\n",
        "            filters=384,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Convolutional Layer #32\n",
        "        # ouput activation map of size 16*16*384\n",
        "        conv32 = tf.layers.conv2d(\n",
        "            inputs=conv31,\n",
        "            filters=384,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            strides=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Pooling Layer #3\n",
        "        # ouput activation map of size 7*7*384\n",
        "        pool3 = tf.layers.max_pooling2d(inputs=conv32, pool_size=[3, 3], strides=2)\n",
        "\n",
        "        # Add branch #3 here\n",
        "        # Convolutional Layer branch3conv\n",
        "        branch3conv = tf.layers.conv2d(\n",
        "            inputs=pool3,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #31\n",
        "        pool3_flat = tf.contrib.layers.flatten(branch3conv)\n",
        "        dense31 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #32\n",
        "        dense32 = tf.layers.dense(inputs=dense31, units=512, activation=tf.nn.relu)\n",
        "\n",
        "    with tf.name_scope(\"branch4\"):    \n",
        "         # Convolutional Layer #41\n",
        "         # ouput activation map of size 3*3*384\n",
        "        conv41 = tf.layers.conv2d(\n",
        "            inputs=pool3,\n",
        "            filters=512,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Convolutional Layer #42\n",
        "        # ouput activation map of size 3*3*384\n",
        "        conv42 = tf.layers.conv2d(\n",
        "            inputs=conv41,\n",
        "            filters=512,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            strides=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        # Pooling Layer #4\n",
        "        # ouput activation map of size 1*1*384\n",
        "        pool4 = tf.layers.max_pooling2d(inputs=conv42, pool_size=[2, 2], strides=2)\n",
        "\n",
        "        # Add branch #4 here\n",
        "        # Convolutional Layer branch4conv\n",
        "        branch4conv = tf.layers.conv2d(\n",
        "            inputs=pool4,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #41\n",
        "        pool4_flat = tf.contrib.layers.flatten(branch4conv)\n",
        "        dense41 = tf.layers.dense(inputs=pool4_flat, units=1024, activation=tf.nn.relu)\n",
        "\n",
        "        #FC layer #42\n",
        "        dense42 = tf.layers.dense(inputs=dense41, units=512, activation=tf.nn.relu)\n",
        "    \n",
        "    with tf.name_scope(\"branch5\"): \n",
        "        # Add main branch here\n",
        "        # Convolutional Layer branchmainconv\n",
        "        branchmainconv = tf.layers.conv2d(\n",
        "            inputs=pool4,\n",
        "            filters=128,\n",
        "            kernel_size=[1, 1],\n",
        "            padding=\"same\",\n",
        "            stride=[1,1],\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        #FC main\n",
        "        pool5_flat = tf.contrib.layers.flatten(branchmainconv)\n",
        "        dense51 = tf.layers.dense(inputs=pool5_flat, units=1024, activation=tf.nn.relu)\n",
        "\n",
        "\n",
        "        #integrate fetures extrated from different branches in main branch\n",
        "        concat=tf.concat([dense11, dense21, dense31,dense41,dense51],1)\n",
        "\n",
        "        #FC layer in main branch\n",
        "        dense52=tf.layers.dense(inputs=concat, units=512, activation=tf.nn.relu)\n",
        "    \n",
        "    V={'B1':dense12, 'B2':dense22, 'B3':dense32, 'B4':dense42, 'B5':dense52}\n",
        "    \n",
        "    return V\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bld22aHbFgM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation_gru(V):\n",
        "    \"\"\" \n",
        "        Arguments:\n",
        "        V -- dictionary of visual features from forward propogation\n",
        "        \n",
        "        Returns:\n",
        "        logits of shape (None,10)\n",
        "        \n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"gru\"): \n",
        "        ht=tf.Variable(tf.zeros(None,512))\n",
        "        ht_=tf.Variable(tf.zeros(None,512))\n",
        "        ht= gru_rnn(V['B1'],V['B2'],V['B3'],V['B4'],V['B5']) # exploiting dependency from local level to global level\n",
        "        ht_=gru_rnn(V['B5'],V['B4'],V['B3'],V['B2'], V['B1']) # exploiting dependency from global level to local level\n",
        "\n",
        "        concat=tf.concat(ht_,ht,1)  #concatination of bi-directional gru\n",
        "\n",
        "        dropout = tf.layers.dropout(inputs=concat, rate=0.4)\n",
        "\n",
        "        logits=tf.layers.dense(inputs=concat2,units=10) \n",
        "    \n",
        "    return logits\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuuPKdC55UCX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_contrastive_loss( V1, V2, label, margin = 1):\n",
        "     \n",
        "    \"\"\" \n",
        "        Arguments:\n",
        "        \n",
        "        label-- 1 if the pair of images are from same category , 0  otherwise\n",
        "        V1, V2 --visual feature from a specific branch for pair of images\n",
        "        \n",
        "        Returns:\n",
        "        loss -- contrastive plus classification loss\n",
        "        \n",
        "    \"\"\"\n",
        "    softmax1= tf.nn.softmax(V1)\n",
        "    softmax2= tf.nn.softmax(V2)\n",
        "    clsL1 = -tf.log(softmax1)\n",
        "    clsL2 = -tf.log(softmax2)\n",
        "    d = tf.reduce_sum(tf.square(V1 - V2), 1)\n",
        "    d_sqrt= tf.sqrt(d)\n",
        "\n",
        "    loss = label * tf.square(tf.maximum(0., margin - d_sqrt)) + (1 - label) * d\n",
        "\n",
        "    loss = 0.5 * tf.reduce_mean(loss)\n",
        "    \n",
        "    loss= loss + clsL1 + clsL2\n",
        "    return loss\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fIMQKsB-To1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_contrastive_loss(V1, V2, label):\n",
        "    '''\n",
        "       Arguments: \n",
        "       V1, V2 -- dictionary of visual features from the branches for pairs of images\n",
        "       label-- 1 if the pair of images are from same category , 0  otherwise\n",
        "    \n",
        "       Returns:\n",
        "       loss-- dictionary of the loss in the branches\n",
        "    '''\n",
        "    B1=cnn_loss(V1['B1'],V2['B1'],label)\n",
        "    B2=cnn_loss(V1['B2'],V2['B2'],label)\n",
        "    B3=cnn_loss(V1['B3'],V2['B3'],label)\n",
        "    B4=cnn_loss(V1['B4'],V2['B4'],label)\n",
        "    B5=cnn_loss(V1['B5'],V2['B5'],label)\n",
        "    \n",
        "    loss={'B1':B1, 'B2':B2, 'B3':B3, 'B4':B4, 'B5':B6}\n",
        "    \n",
        "    return loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7sNdfSA_MYd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cnn_cost(V, Y):\n",
        "    with tf.name_scope(\"branch1\"): \n",
        "      z1=tf.contrib.layers.fully_connected(V['B1'], 8,activation_fn=None)\n",
        "      cost1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z1, labels=Y)) \n",
        "    \n",
        "    with tf.name_scope(\"branch2\"):\n",
        "      z2=tf.contrib.layers.fully_connected(V['B2'], 8,activation_fn=None)\n",
        "      cost2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z2, labels=Y))\n",
        "    \n",
        "    with tf.name_scope(\"branch3\"):\n",
        "      z3=tf.contrib.layers.fully_connected(V['B3'], 8,activation_fn=None)\n",
        "      cost3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=Y))\n",
        "    \n",
        "    with tf.name_scope(\"branch4\"):\n",
        "      z4=tf.contrib.layers.fully_connected(V['B4'], 8,activation_fn=None)\n",
        "      cost4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z4, labels=Y))\n",
        "    \n",
        "    with tf.name_scope(\"branch5\"):\n",
        "      z5=tf.contrib.layers.fully_connected(V['B5'], 8,activation_fn=None)\n",
        "      cost5 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z5, labels=Y))   \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    cost={'cost1':cost1, 'cost2':cost2, 'cost3':cost4, 'cost4':cost4, 'cost5':cost5}\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7KbaRHoyD5K6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_gru_loss(logits, Y):\n",
        "   \"\"\" \n",
        "        Arguments:\n",
        "        logits : classification layer\n",
        "        Y -- ground truth\n",
        "        \n",
        "        Returns:\n",
        "        cost -- contrastive plus classification loss\n",
        "        \n",
        "    \"\"\"\n",
        "  with tf.name_scope(\"gru\"):\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
        "  return cost\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAt5mxPM7l2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    with tf.device('/gpu:0'):\n",
        "        m = X.shape[0]                  # number of training examples\n",
        "        mini_batches = []\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Step 1: Shuffle (X, Y)\n",
        "        permutation = list(np.random.permutation(m))\n",
        "        shuffled_X = X[permutation,:,:,:]\n",
        "        shuffled_Y = Y[permutation,:]\n",
        "\n",
        "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "        for k in range(0, num_complete_minibatches):\n",
        "            mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
        "            mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
        "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "            mini_batches.append(mini_batch)\n",
        "\n",
        "        # Handling the end case (last mini-batch < mini_batch_size)\n",
        "        if m % mini_batch_size != 0:\n",
        "            mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
        "            mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
        "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "            mini_batches.append(mini_batch)\n",
        "\n",
        "        return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arWpzZwbzZ7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, learning_rate=0.009,num_epochs=10, minibatch_size=64, print_cost=True):\n",
        "    with tf.device('/gpu:0'):\n",
        "        (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
        "        n_y = Y_train.shape[1]  \n",
        "        #print(Y_train.shape)\n",
        "                                  \n",
        "        X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
        "        \n",
        "        V=fowarwad_propogation_cnn(X)\n",
        "        \n",
        "        cost=compute_cnn_cost(V, Y)\n",
        "        \n",
        "        \n",
        "        ''' optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost['cost1']) \n",
        "        optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost['cost2'])\n",
        "        optimizer3 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost['cost3'])\n",
        "        optimizer4 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost['cost4'])\n",
        "        optimizer5 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost['cost5'])\n",
        "        '''\n",
        "        \n",
        "        optimizer = tf.train.AdagradOptimzer(learning_rate=learning_rate)\n",
        "\n",
        "        first_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"branch1\")\n",
        "        first_train_op = optimizer.minimize(cost['cost1'], var_list=first_train_vars)\n",
        "\n",
        "        second_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"branch2\")                     \n",
        "        second_train_op = optimizer.minimize(cost['cost2'], var_list=second_train_vars)\n",
        "        \n",
        "        third_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"branch3\")                     \n",
        "        third_train_op = optimizer.minimize(cost['cost3'], var_list=third_train_vars)\n",
        "        \n",
        "        fourth_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"branch4\")                     \n",
        "        fourth_train_op = optimizer.minimize(cost['cost4'], var_list=fourth_train_vars)\n",
        "        \n",
        "        main_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"branch5\")                     \n",
        "        main_train_op = optimizer.minimize(cost['cost5'], var_list=main_train_vars)\n",
        "        \n",
        "        G=forward_propagation_gru(V)\n",
        "        loss=compute_gru_loss(G, Y)\n",
        "        \n",
        "        gru_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"gru\")\n",
        "        gru_train_op = optimizer.minimize(loss, var_list=gru_train_vars )\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "            #costs = [] \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([first_train_op, cost['cost1'] ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    #costs.append(minibatch_cost)        \n",
        "            #plt.plot(np.squeeze(costs))\n",
        "            #plt.ylabel('cost')\n",
        "            #plt.xlabel('iterations (per tens)')\n",
        "            #plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "            #plt.show()\n",
        "            \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([second_train_op, cost['cost2'] ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    #costs.append(minibatch_cost)\n",
        "            \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([third_train_op, cost['cost3'] ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    #costs.append(minibatch_cost)\n",
        "            \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([fourth_train_op, cost['cost4'] ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    #costs.append(minibatch_cost)\n",
        "            \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([main_train_op, cost['cost5'] ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    #costs.append(minibatch_cost)\n",
        "            \n",
        "            \n",
        "            costs = [] \n",
        "            for epoch in range(num_epochs):\n",
        "                minibatch_cost = 0.\n",
        "                num_minibatches = int(m / minibatch_size)\n",
        "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size,0)\n",
        "                for minibatch in minibatches:\n",
        "                    (minibatch_X, minibatch_Y) = minibatch\n",
        "                    #print(minibatch_Y.shape)\n",
        "                    _ , temp_cost = sess.run([gru_train_op, loss ], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                    minibatch_cost += temp_cost / num_minibatches\n",
        "                if print_cost == True and epoch % 5 == 0:\n",
        "                    print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
        "                if print_cost == True and epoch % 1 == 0:\n",
        "                    costs.append(minibatch_cost)\n",
        "            plt.plot(np.squeeze(costs))\n",
        "            plt.ylabel('cost')\n",
        "            plt.xlabel('iterations (per tens)')\n",
        "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "            plt.show()        \n",
        "\n",
        "            predict_op = tf.argmax(G, 1)\n",
        "            correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
        "\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "            print(accuracy)\n",
        "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
        "            predict_test =  predict_op.eval({X: X_test})\n",
        "            \n",
        "            print(\"Train Accuracy:\", train_accuracy)\n",
        "            #print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "            return predict_test, train_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaqMOFRPC9rl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict,_ = model(X_train, Y_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}